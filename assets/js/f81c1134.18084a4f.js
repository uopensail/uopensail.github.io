"use strict";(self.webpackChunkuopensail_github_io=self.webpackChunkuopensail_github_io||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/AppS/DeepFM","metadata":{"permalink":"/blog/AppS/DeepFM","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/AppS/DeepFM.md","source":"@site/blog/AppS/DeepFM.md","title":"Application of DeepFM Model in AppS","description":"In the field of recommendation systems, efficiently combining low-order and high-order feature interactions to improve prediction accuracy has always been a key challenge. The DeepFM model offers a solution that combines memory capacity and generalization ability by integrating Factorization Machines (FM) with Deep Neural Networks (DNN). This article will introduce the application and effectiveness of DeepFM in the AppS business.","date":"2025-03-22T08:54:54.000Z","tags":[],"readingTime":4.365,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Application of the ESMM Model in AppS","permalink":"/blog/AppS/ESMM"}},"content":"In the field of recommendation systems, efficiently combining low-order and high-order feature interactions to improve prediction accuracy has always been a key challenge. The DeepFM model offers a solution that combines memory capacity and generalization ability by integrating Factorization Machines (FM) with Deep Neural Networks (DNN). This article will introduce the application and effectiveness of DeepFM in the AppS business.\\n\\n<img src=\\"../../static/images/deepfm.svg\\" title=\\"\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n## Introduction\\n\\nDeepFM (Deep Factorization Machine) is a recommendation system model that combines factorization machines (FM) with deep learning. It aims to capture both low-order and high-order feature interactions simultaneously. The architecture of DeepFM consists of two components: the FM component and the Deep component. The FM component is used to capture low-order feature interactions, while the Deep component learns high-order feature interactions through a multi-layer perceptron (MLP).\\n\\n### FM Module\\n\\n- **Function**: The FM module focuses on capturing second-order interactions between features. It leverages feature embeddings to compute interaction terms and efficiently represent relationships between sparse features.\\n- **Advantage**: By modeling low-order interactions, the FM module effectively handles sparse data, making it particularly suitable for scenarios with a large number of sparse features.\\n\\n### DNN Module\\n\\n- **Function**: The DNN module is used to learn high-order feature combinations. Through a multi-layer neural network, DNN can capture complex nonlinear feature interactions.\\n- **Customization Capability**: Users can design the network structure of the DNN according to specific needs, including the number of layers, the number of neurons in each layer, activation functions, and regularization strategies.\\n- **Advantage**: With a flexible structure design, the DNN module can generalize to new feature combinations and improve the model\'s adaptability to different data distributions.\\n\\n### Benefits of DeepFM over FM\\n\\n1. **Comprehensive Feature Interaction Capability**: Traditional FM models mainly focus on second-order interactions between features, whereas DeepFM can effectively capture high-order feature interactions by introducing a deep learning component, thus improving recommendation accuracy.\\n\\n2. **No Need for Manual Feature Engineering**: DeepFM can automatically learn feature interactions, reducing the reliance on manual feature engineering, which is particularly useful for handling complex, large-scale datasets.\\n\\n3. **Shared Feature Embeddings**: The feature embedding layer in DeepFM is shared between the FM and Deep components, making the model more efficient in capturing feature interactions while reducing the number of model parameters.\\n\\n### Advantages of DeepFM\\n\\n- **Comprehensive Capability**: DeepFM combines the strengths of FM and DNN, allowing it to learn both low-order and high-order feature interactions without the need for feature engineering.\\n- **Model Simplicity**: Compared to training FM and DNN separately and then combining them, DeepFM maintains model compactness and efficiency by sharing the feature embedding layer.\\n- **Wide Applicability**: Due to its flexibility and strong expressive power, DeepFM is widely used in fields such as ad click-through rate prediction and recommendation systems.\\n\\n### Example Code for Developing DeepFM with PyTorch\\n\\nIn the following example, we will develop the FM and DNN modules separately and then combine them into a complete DeepFM model.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass DeepFM(nn.Module):\\n    def __init__(self, field_dims, embed_dim, mlp_dims):\\n        super(DeepFM, self).__init__()\\n        self.embeddings = nn.ModuleList([\\n            nn.Embedding(dim, embed_dim) for dim in field_dims\\n        ])\\n        self.linear = nn.Linear(sum(field_dims), 1)\\n        self.fm = FM(embed_dim)\\n        self.dnn = DNN(sum(field_dims) * embed_dim, mlp_dims)\\n\\n    def forward(self, x):\\n        x_emb = [emb(x[:, i]) for i, emb in enumerate(self.embeddings)]\\n        x_emb = torch.cat(x_emb, dim=1)\\n        x_linear = self.linear(x)\\n        x_fm = self.fm(x_emb)\\n        x_dnn = self.dnn(x_emb.view(x_emb.size(0), -1))\\n        return x_linear + x_fm + x_dnn\\n\\nclass FM(nn.Module):\\n    def __init__(self, embed_dim):\\n        super(FM, self).__init__()\\n        self.embed_dim = embed_dim\\n\\n    def forward(self, x):\\n        square_of_sum = torch.sum(x, dim=1) ** 2\\n        sum_of_square = torch.sum(x ** 2, dim=1)\\n        return 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)\\n\\nclass DNN(nn.Module):\\n    def __init__(self, input_dim, dims):\\n        super(DNN, self).__init__()\\n        layers = []\\n        for dim in dims:\\n            layers.append(nn.Linear(input_dim, dim))\\n            layers.append(nn.ReLU())\\n            input_dim = dim\\n        self.layers = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        return self.layers(x)\\n\\n# Example usage:\\nfield_dims = [10, 10, 10]  # Example field dimensions\\nembed_dim = 10\\nmlp_dims = [64, 32]\\nmodel = DeepFM(field_dims, embed_dim, mlp_dims)\\n\\n# Dummy input\\nx = torch.randint(0, 10, (4, len(field_dims)))  # Batch size 4\\noutput = model(x)\\nprint(output)\\n```\\n\\n## Application\\n\\n### 1. Feature Embedding Configuration\\n\\nIn our DeepFM model, the embedding dimension for each feature is set to 10. This configuration effectively captures low-order feature interactions and provides a solid foundation for subsequent high-order feature learning through the deep neural network.\\n\\n### 2. Model Training and Optimization\\n\\nBuilding on our experience with FM model training, the DeepFM model excels in combining memory and generalization. The FM component captures low-order feature interactions, while the DNN component learns high-order feature combinations. This combination achieves excellent results in the current business scenario.\\n\\n- Memory Capability: DeepFM uses the FM component\'s low-order interactions to capture known, stable feature combinations.\\n\\n- Generalization Capability: Through the DNN component, DeepFM can discover new, potential high-order feature combinations, enhancing the prediction of user behavior.\\n\\n### 3. AB Testing Results\\n\\nIn the \\"Guess You Like\\" module, deploying the DeepFM model led to a **4.66%** increase in average distribution per user. This result indicates that DeepFM significantly enhances the quality of personalized recommendations for users.\\n\\n<img src=\\"../../static/images/DeepFM-AB.png\\" title=\\"\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n## Further Reading\\n\\n- [A Factorization-Machine based Neural Network for CTR Prediction - arXiv](https://arxiv.org/abs/1703.04247)\\n\\n- [Deep Factorization Machines \u2014 Dive into Deep Learning](https://d2l.ai/chapter_recommender-systems/deepfm.html)\\n\\n- [DeepFM for recommendation systems explained with codes](https://medium.com/data-science-in-your-pocket/deepfm-for-recommendation-systems-explained-with-codes-c200063990f7)"},{"id":"/AppS/ESMM","metadata":{"permalink":"/blog/AppS/ESMM","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/AppS/ESMM.md","source":"@site/blog/AppS/ESMM.md","title":"Application of the ESMM Model in AppS","description":"In modern recommendation systems, particularly within the AppS business environment, predicting user behaviors such as Click-Through Rate (CTR) and Conversion Rate (CVR) is crucial for enhancing user satisfaction and driving business growth. The ESMM model, with its unique architecture and efficient multi-task learning capability, offers an outstanding solution for the AppS business.","date":"2025-03-22T08:54:54.000Z","tags":[],"readingTime":3.81,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Application of DeepFM Model in AppS","permalink":"/blog/AppS/DeepFM"},"nextItem":{"title":"Application of FM Model in AppS","permalink":"/blog/AppS/FM"}},"content":"In modern recommendation systems, particularly within the AppS business environment, predicting user behaviors such as Click-Through Rate (CTR) and Conversion Rate (CVR) is crucial for enhancing user satisfaction and driving business growth. The ESMM model, with its unique architecture and efficient multi-task learning capability, offers an outstanding solution for the AppS business.\\n\\n<img title=\\"\\" src=\\"../../static/images/ESMM-origin.webp\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n## Introduction\\n\\nESMM, short for Entire Space Multi-task Model, is a multi-task learning model specifically designed to tackle problems related to ad recommendations and user behavior prediction. The core idea of ESMM is to enhance the overall performance of the model by simultaneously learning multiple related tasks. This approach not only shares potential information between different tasks but also effectively alleviates the issue of data sparsity.\\n\\nThe ESMM model is typically applied to predict CTR and CVR. Traditional methods often train two separate models to predict CTR and CVR, whereas ESMM simultaneously performs these two prediction tasks within a unified framework, thereby capturing the correlation between them more effectively.\\n\\nTo learn more about the foundational concepts of ESMM, you can read this\xa0[academic paper on ESMM](https://arxiv.org/abs/1804.07931).\\n\\n### Major Advantages of ESMM\\n\\n- **Data Efficiency**: By sharing the feature space, ESMM can better utilize data, especially in sparse data scenarios.\\n- **Performance Enhancement**: By jointly learning multiple tasks, ESMM can better capture the mutual influences between related tasks, improving the accuracy of predictions.\\n- **Simplified Architecture**: Compared to training multiple models independently, ESMM provides a more streamlined and efficient solution.\\n\\n### Differences Between ESMM and MMOE\\n\\nIn multi-task learning, besides ESMM, there is another popular model known as MMOE (Multi-gate Mixture-of-Experts). Both MMOE and ESMM aim to enhance the performance of multiple tasks by sharing information, but they exhibit significant differences in architecture and application scenarios:\\n\\n#### Architectural Differences\\n\\n- **ESMM**: ESMM conducts multi-task learning by sharing the entire feature space. It primarily uses a unified network structure to simultaneously predict multiple tasks (such as CTR and CVR) and enhances overall performance by sharing underlying features.\\n\\n- **MMOE**: MMOE employs a more complex structure by introducing multiple expert networks and gating mechanisms to dynamically select suitable features and model paths for each task. Each task has its own gating network to select the most relevant information from multiple experts.\\n\\n#### Application Scenarios\\n\\n- **ESMM**: Suitable for scenarios where tasks are highly related and require extensive information sharing, particularly when data is sparse and efficient information utilization is needed.\\n\\n- **MMOE**: More flexible and applicable to scenarios where task correlations are weaker or personalized feature selection is required. Due to its complex selection mechanism, MMOE performs better in situations with conflicting task requirements.\\n\\n#### Performance Aspects\\n\\n- **ESMM**: Provides stable performance improvements between related tasks through its simplified network architecture and efficient feature sharing.\\n\\n- **MMOE**: Capable of offering higher prediction accuracy in complex task environments through flexible expert selection mechanisms, especially when task requirements are diverse.\\n\\n## Application\\n\\n### Similarity in Basic Structure Between ESMM and MMOE\\n\\nThe ESMM model shares many structural similarities with the traditional MMOE model. Both employ a multi-task learning framework to enhance the performance of different tasks by sharing information. However, the ESMM model adopts a different approach in the final conversion rate prediction: it calculates the predicted Conversion Rate (pCVR) as the product of two towers, a design aimed at fully capturing the interaction between CTR and CVR.\\n\\n### Key Components of the ESMM Model\\n\\n<img title=\\"\\" src=\\"../../static/images/ESMM.webp\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n#### Two Expert Networks\\n\\nWhen applied to the AppS business, ESMM uses two expert networks. These expert networks are responsible for handling features related to CTR and CVR tasks, respectively. Through specialized network structures, ESMM can better extract and utilize task-specific information, thereby enhancing prediction accuracy.\\n\\n#### Two Gating Mechanisms\\n\\nIn addition to expert networks, ESMM also employs two gating mechanisms to control the CTR and CVR tasks separately. These gating mechanisms dynamically adjust the selection and utilization of features for each task, ensuring that each task receives the most suitable information flow. Through optimization of gating mechanisms, ESMM provides more precise results in complex user behavior predictions.\\n\\n## Experimental Results and Effects\\n\\nIn practical applications within the AppS business, the ESMM model has demonstrated significant results through A/B testing. In the \\"Guess You Like\\" module, the ESMM model successfully achieved a 6.45% increase in average distribution per user.\\n\\n<img title=\\"\\" src=\\"../../static/images/ESMM-AB.png\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n## Further Reading\\n\\n- [Entire Space Multi-Task Model: An Effective Approach for Estimating ... - arXiv](https://arxiv.org/abs/1804.07931)\\n\\n- [ESMM &mdash; easy_rec 0.8.5 documentation](https://easyrec.readthedocs.io/en/latest/models/esmm.html)\\n\\n- [GitHub - dai08srhg/ESMM: PyTorch implementation of Entire Space Multitask Model (ESMM)](https://github.com/dai08srhg/ESMM)"},{"id":"/AppS/FM","metadata":{"permalink":"/blog/AppS/FM","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/AppS/FM.md","source":"@site/blog/AppS/FM.md","title":"Application of FM Model in AppS","description":"Introduction","date":"2025-03-22T08:54:54.000Z","tags":[],"readingTime":4.62,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Application of the ESMM Model in AppS","permalink":"/blog/AppS/ESMM"},"nextItem":{"title":"Application of the MMOE Model in AppS","permalink":"/blog/AppS/MMOE"}},"content":"## Introduction\\n\\nFactorization Machines (FM) are powerful machine learning models, especially widely used in recommendation systems and advertising click-through rate prediction. FM models can effectively capture the cross information between features and are highly efficient and easy to implement in engineering.\\n\\n<img title=\\"\\" src=\\"../../static/images/FM.jpg\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n### 1. Advantages of FM over Linear Regression (LR)\\n\\nLinear Regression (LR) is a simple and intuitive model, but it cannot capture the cross and nonlinear relationships between features. The FM model introduces latent vectors to factorize features, effectively capturing second-order interactions between features. Compared to LR, FM has the following advantages:\\n\\n- **Feature Interaction**: FM can automatically learn interactions between features without manually constructing cross-features, leading to better performance on complex datasets.\\n- **Model Flexibility**: FM performs well in highly sparse datasets and is suitable for scenarios with a large number of categorical features.\\n- **Strong Generalization**: By factorizing features, FM can avoid overfitting, especially when the data is high-dimensional but with a small sample size.\\n\\n### 2. Time Complexity Analysis of FM\\n\\nA significant advantage of the FM model is its efficient computational capability. Although FM considers interactions between all pairs of features, its time complexity remains\xa0($O(N)$)\xa0rather than\xa0($O(N\u200b^2\u200b\u200b)$). This is because FM simplifies the calculation of feature interactions through factorization as follows:\\n\\n$\\\\hat{y}(x) = w_0 + \\\\sum_{i=1}^{N}w_ix_i + \\\\sum_{i=1}^{N}\\\\sum_{j=i+1}^{N} \\\\langle v_i, v_j \\\\rangle x_ix_j$\\n\\nwhere\xa0(\u27e8v\u200bi\u200b\u200b,v\u200bj\u200b\u200b\u27e9)\xa0denotes the inner product of two feature latent vectors, significantly reducing computational cost. The steps are as follows:\\n\\n- **Linear Term**:\xa0($\\\\sum_{i=1}^{N}w_ix_i$), with a time complexity of\xa0($O(N)$).\\n- **Interaction Term**: Through factorization,\xa0($\\\\sum_{i=1}^{N}\\\\sum_{j=i+1}^{N} \\\\langle v_i, v_j \\\\rangle x_ix_j$)\xa0can be computed in\xa0($O(N)$)\xa0time.\\n\\nThis efficiency allows FM to maintain fast computation speeds even when handling large-scale datasets.\\n\\n### 3. Ease of Engineering Implementation\\n\\nThe FM model not only has theoretical advantages but is also relatively simple to implement in practice, especially when developed and deployed using deep learning frameworks like PyTorch. Below is a brief explanation and example code of implementing FM using PyTorch:\\n\\n### Developing and Training FM Model with PyTorch\\n\\nDeveloping an FM model with PyTorch is very straightforward, as PyTorch\'s flexibility and ease of use make customizing model structures and training processes simple. Here is a simplified implementation example of an FM model:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\n\\nclass FactorizationMachine(nn.Module):\\n    def __init__(self, n_features, k):\\n        super(FactorizationMachine, self).__init__()\\n        self.linear = nn.Linear(n_features, 1)\\n        self.v = nn.Parameter(torch.randn(n_features, k))\\n\\n    def forward(self, x):\\n        linear_part = self.linear(x)\\n        interaction_part = 0.5 * torch.sum(\\n            torch.pow(torch.mm(x, self.v), 2) - torch.mm(torch.pow(x, 2), torch.pow(self.v, 2)), dim=1, keepdim=True\\n        )\\n        return linear_part + interaction_part\\n\\n# Example usage\\nn_features = 10  # Assuming 10 features\\nk = 5  # Dimension of latent vectors\\n\\nmodel = FactorizationMachine(n_features, k)\\ncriterion = nn.MSELoss()\\noptimizer = optim.SGD(model.parameters(), lr=0.01)\\n\\n# Assuming we have some training data\\nX_train = torch.randn(100, n_features)  # 100 samples\\ny_train = torch.randn(100, 1)\\n\\n# Training process\\nmodel.train()\\nfor epoch in range(100):  # Train for 100 epochs\\n    optimizer.zero_grad()\\n    outputs = model(X_train)\\n    loss = criterion(outputs, y_train)\\n    loss.backward()\\n    optimizer.step()\\n\\n    if (epoch+1) % 10 == 0:\\n        print(f\'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}\')\\n```\\n\\nThrough the above code example, it is evident that implementing the FM model with PyTorch is intuitive and flexible. Users can easily adjust model structures, optimizers, and loss functions to quickly adapt to different business needs.\\n\\n## Application\\n\\n### 1. Feature Embedding Configuration\\n\\nIn our FM model, the embedding dimension for each feature is set to 10. This configuration allows us to fully capture the interactions between features without adding excessive computational overhead. Choosing the right embedding dimension is a critical step in balancing model complexity and computational efficiency.\\n\\n### 2. Addressing Sample Imbalance Issues\\n\\nDuring the model training process, we found a significant imbalance in the ratio of positive to negative samples, primarily due to the substantial differences in homepage exposure click data. To address this issue, we conducted two experiments:\\n\\n- **Random Negative Sample Dropping**: This straightforward method aims to balance the sample ratio by reducing the number of negative samples. However, our experiments showed that while this approach can somewhat alleviate the imbalance issue, it does not significantly improve model performance.\\n\\n- **Positive Sample Weighting**: In contrast, we applied weighting to positive samples, giving them higher learning importance. This method better emphasizes genuine user interest behaviors. By adjusting sample weights, we effectively increased the model\'s focus on positive samples, leading to a significant improvement in overall model performance.\\n\\nThe experiments demonstrated that positive sample weighting outperforms random negative sample dropping. This is mainly because weighting allows us to more accurately capture user interest preferences, avoiding model bias caused by an excess of negative samples.\\n\\n### 3. Data Issues and Model Optimization\\n\\nDuring the initial model training, we encountered a critical issue: training with the full dataset did not achieve the expected results. Upon thorough investigation, we discovered that many homepage exposures used frontend caching due to network issues. These cached data were not user-initiated actions, thus interfering with the model\'s learning process.\\n\\nTo resolve this issue, we differentiated the reported data on the first screen, removing the cached data. This approach ensured that the model was trained using genuine user behavior data, ultimately ensuring the effectiveness of the model once deployed.\\n\\n### 4. AB Testing Results\\n\\nThe effect of online weighting was verified through AB testing. Specific AB test screenshots will be presented here, further proving the effectiveness of our optimization strategies in practical applications. In the **Guess You Like** section on the homepage, the average distribution per person increased by **14.8%**.\\n\\n<img title=\\"\\" src=\\"../../static/images/FM-AB.webp\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n## Further Reading\\n\\n- [Factorization Machine models in PyTorch - GitHub](https://github.com/rixwew/pytorch-fm)\\n\\n- [Factorization Machines](https://d2l.ai/chapter_recommender-systems/fm.html)"},{"id":"/AppS/MMOE","metadata":{"permalink":"/blog/AppS/MMOE","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/AppS/MMOE.md","source":"@site/blog/AppS/MMOE.md","title":"Application of the MMOE Model in AppS","description":"In the AppS business, recommendation systems need not only to improve user Click-Through Rate (CTR) but also to enhance Conversion Rate (CVR) to achieve comprehensive user engagement and business growth. The Multi-gate Mixture-of-Experts (MMOE) model offers an efficient solution by simultaneously optimizing multiple objectives to meet these business needs.","date":"2025-03-22T08:54:54.000Z","tags":[],"readingTime":4.76,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Application of FM Model in AppS","permalink":"/blog/AppS/FM"},"nextItem":{"title":"The One Epoch Phenomenon in Recommendation Algorithms","permalink":"/blog/AppS/OneEpochInRecAlgo"}},"content":"In the AppS business, recommendation systems need not only to improve user Click-Through Rate (CTR) but also to enhance Conversion Rate (CVR) to achieve comprehensive user engagement and business growth. The Multi-gate Mixture-of-Experts (MMOE) model offers an efficient solution by simultaneously optimizing multiple objectives to meet these business needs.\\n\\n<img title=\\"\\" src=\\"../../static/images/MMOE-origin.webp\\" alt=\\"\\" data-align=\\"center\\" width=\\"522\\">\\n\\n## Introduction\\n\\nIn the field of recommendation systems and advertising, models often need to optimize multiple objectives simultaneously, such as Click-Through Rate (CTR) and Conversion Rate (CVR). The Multi-gate Mixture-of-Experts (MMOE) model provides an effective solution by achieving better goal synergy optimization within a multi-task learning framework.\\n\\n### 1. What is MMOE?\\n\\nMMOE (Multi-gate Mixture-of-Experts) is a deep learning architecture for multi-task learning, designed to simultaneously optimize multiple related but distinct objectives. It introduces multiple \\"expert\\" networks and \\"gating\\" mechanisms to dynamically select and combine different expert outputs to meet the needs of different tasks.\\n\\n#### Core Components\\n\\n- **Expert Networks**: Multiple sub-networks, each responsible for learning different representations of input features to meet the needs of different tasks.\\n- **Gating Networks**: For each task, MMOE introduces an independent gating network responsible for selecting the appropriate combination of experts for the input sample. The gating network dynamically allocates weights to each expert based on input features.\\n\\n#### Working Principle\\n\\nMMOE combines expert networks through the gating mechanism, enabling the model to flexibly select the most suitable feature combinations for each task while sharing basic features. This mechanism allows for the sharing of information between tasks while mitigating negative transfer effects.\\n\\n### 2. Why Use MMOE?\\n\\nIn business scenarios, especially in advertising and recommendation systems, it is often necessary to optimize multiple key metrics simultaneously. For example, improving Click-Through Rate (CTR) and Conversion Rate (CVR) are common business objectives. Traditional single-task models often struggle to balance these goals, while MMOE offers an ideal solution for multi-objective optimization.\\n\\n#### Business Goals: Considering CTR and CVR\\n\\n- **Improving CTR (Click-Through Rate)**: CTR is a metric that measures the ability of advertisements or recommendations to attract user clicks. Increasing CTR can directly enhance user interaction and engagement.\\n\\n- **Improving CVR (Conversion Rate)**: CVR is a metric that measures the ability of users to complete target behaviors (such as purchases, registrations, etc.). Enhancing CVR can directly impact the business\'s final revenue.\\n\\n#### Why Choose MMOE?\\n\\n- **Task Synergy Optimization**: MMOE allows for the simultaneous optimization of CTR and CVR objectives by sharing feature representations of expert networks and using independent gating mechanisms.\\n- **Reducing Negative Transfer**: Through the combination of experts and gating, MMOE effectively reduces negative transfer effects between tasks, ensuring that optimizing one objective does not significantly harm the other.\\n- **Dynamic Adaptability**: The dynamic gating mechanism of MMOE enables the model to adjust expert combinations in real-time based on input features, adapting to different user behavior patterns and preferences.\\n\\nMMOE demonstrates its strong adaptability and optimization performance in multi-task learning. For scenarios that require simultaneous consideration of multiple business goals, such as optimizing ad CTR and CVR, MMOE provides an efficient and flexible solution. By designing expert and gating structures wisely, MMOE can achieve more refined recommendation and advertising strategies in complex business environments.\\n\\n## Application\\n\\n### Model Architecture and Strategy\\n\\n#### 1. Retaining FM Cross Strategy\\n\\nIn the MMOE architecture, we continue to retain the original Factorization Machine (FM) cross strategy. This strategy excels in modeling low-order feature interactions, effectively capturing the basic relationship between users and content. We integrate FM\'s output into the final output of MMOE to ensure full utilization of basic feature interaction information.\\n\\n#### 2. Expert Network Design\\n\\nThe MMOE model employs two expert networks, each focusing on learning different feature combinations. Through diversified expert network design, the model can capture user behavior patterns from different perspectives and enhance adaptability to complex data.\\n\\n#### 3. Gating Mechanism\\n\\nWe designed two independent gating networks responsible for optimizing CTR and CVR, respectively:\\n\\n- **CTR Gating**: Controls the expert combination related to click-through rate features, ensuring that recommended content can attract user clicks.\\n- **CVR Gating**: Optimizes the feature combination related to download conversion, increasing the probability of users downloading applications.\\n\\n#### 4. Training Strategy\\n\\nDuring model training, we use click and download behaviors as task labels and design a fixed-weight loss function:\\n\\n- **Loss Weight Allocation**:\\n  - The loss weight for PCTR is set to 0.95, emphasizing the optimization of click behavior.\\n  - The loss weight for PCTCVR is set to 0.05, ensuring that download behavior receives appropriate attention.\\n\\nThis weight allocation ensures that CTR is the primary optimization direction while also considering the CVR objective.\\n\\n<img title=\\"\\" src=\\"../../static/images/mmoe.webp\\" alt=\\"\\" data-align=\\"center\\" width=\\"445\\">\\n\\n#### 5. Online Inference and Ranking\\n\\nDuring online inference, we apply the same weights to PCTR and PCVR and rank them based on the weighted scores. Through this strategy, we can balance the priorities of clicks and downloads in recommendation ranking, optimizing user experience and business metrics.\\n\\n### AB Testing Results\\n\\nBy applying the MMOE model in the \\"Guess You Like\\" module, our AB testing results showed a **13.1%** increase in average distribution per user. This significant improvement validates the effectiveness of the MMOE model in simultaneously optimizing CTR and CVR, bringing higher user engagement and conversion rates to the AppS business.\\n\\n<img title=\\"\\" src=\\"../../static/images/MMOE-AB.png\\" alt=\\"\\" width=\\"522\\" data-align=\\"center\\">\\n\\n### Conclusion\\n\\nThe MMOE model achieves comprehensive optimization of CTR and CVR in the AppS business through its flexible expert and gating mechanisms. Combined with the FM cross strategy, MMOE not only enhances the predictive ability of recommendation systems but also improves multi-objective synergy optimization of user behavior, providing strong support for business development.\\n\\n## Further Reading\\n\\n- [Modeling Task Relationships in Multi-task Learning with\\n  Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)\\n\\n- [The Annotated Multi-Task Ranker: An MMoE Code Example](https://www.yuan-meng.com/posts/mtml/)"},{"id":"/AppS/OneEpochInRecAlgo","metadata":{"permalink":"/blog/AppS/OneEpochInRecAlgo","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/AppS/OneEpochInRecAlgo.md","source":"@site/blog/AppS/OneEpochInRecAlgo.md","title":"The One Epoch Phenomenon in Recommendation Algorithms","description":"In the field of Click-Through Rate (CTR) prediction and recommendation systems, the \\"ONE EPOCH phenomenon\\" refers to the overfitting phenomenon where deep models achieve optimal generalization performance after only one epoch of training, and continued training leads to a sharp decline in performance on the test set.","date":"2025-03-22T08:54:54.000Z","tags":[],"readingTime":2.315,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Application of the MMOE Model in AppS","permalink":"/blog/AppS/MMOE"},"nextItem":{"title":"Applicaton Store(AppS) Introduction","permalink":"/blog/AppS/intro"}},"content":"In the field of Click-Through Rate (CTR) prediction and recommendation systems, the \\"ONE EPOCH phenomenon\\" refers to the overfitting phenomenon where deep models achieve optimal generalization performance after only one epoch of training, and continued training leads to a sharp decline in performance on the test set.\\n\\n## Phenomenon Characteristics\\n\\n- Performance During Training\u200c\\n  \\nThe model reaches a peak AUC on the test set at the end of the first epoch, and performance rapidly declines after the start of the second epoch.\\nOverfitting manifests as a continuous decrease in training loss, but validation set metrics (such as AUC) significantly deteriorate after one epoch.\\n\\n- Relevance to Industrial Practice\u200c\\n  \\nThis phenomenon explains why many industrial-grade recommendation systems stop training after only one epoch of data, rather than following the multiple iterations typical of traditional deep learning.\\n<center>\\n<img title=\\"\\" src=\\"../../static/images/one-epoch-1.png\\" alt=\\"\\" width=\\"268\\" data-align=\\"center\\">\\n<img title=\\"\\" src=\\"../../static/images/one-epoch-2.png\\" alt=\\"\\" width=\\"261\\" data-align=\\"center\\">\\n</center>\\n\\n## Key Causes\\n\\n- Feature Sparsity\u200c\\n\\n**The high number of IDs in feature domains** (such as user IDs, product IDs) results in extremely low frequencies of occurrence for each ID, making the model prone to memorizing sparse samples rather than generalizing.\\nThe Embedding layer corresponding to sparse features overfits low-frequency IDs during multiple training epochs.\\n\\n- Model Optimization Characteristics\u200c\\n\\n**Strong optimizers (such as Adam) and larger learning rates** accelerate convergence, enabling the model to complete effective learning within one epoch, with subsequent training falling into local overfitting.\\nComplex model structures (such as deep networks) with excessive capacity exacerbate the memorization of noise/sparse features.\\n\\n## Mitigation Strategies\\n\\n- Feature Engineering Optimization\u200c\\n\\nReduce Sparsity\u200c: Merge low-frequency IDs, use default values to replace sparse IDs, reduce hash space.\\nDynamic Feature Filtering\u200c: Dynamically adjust the participation intensity of sparse features based on the training stage.\\n\\n- Training Parameter Adjustment\u200c\\n\\nLimit Training Epochs\u200c: Force training to stop after only one epoch.\\nLearning Rate Control\u200c: Use a smaller initial learning rate or a learning rate decay strategy.\\n\\n- Model Structure Improvement\u200c\\n\\nSimplify the Embedding layer or introduce regularization (such as Dropout) to suppress overfitting.\\n\\n## Insights from Industrial Practice\\n\\n- Strong Correlation with Data Distribution\u200c\\n\\nThis phenomenon is particularly significant in high-sparsity business scenarios (such as advertising recommendations, e-commerce CTR prediction), but rarely occurs in dense data tasks.\\n\\n- Priority of Optimization Directions\u200c\\n\\nThe industrial community tends to directly avoid the problem through feature engineering and early stopping strategies, rather than modifying the model structure.\\n\\n## Theoretical Hypotheses\\n\\nSome studies suggest that the ONE EPOCH phenomenon may stem from the dynamic balance between sample memorization and generalization: in the first epoch, the model completes coarse-grained learning of key features, and subsequent training disrupts generalization ability due to over-refinement of local features.\\n\\n## Further Reading\\n[Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Prediction Models](https://arxiv.org/pdf/2209.06053)\\n\\n[Ali\'s OneEpoch VS KuaiShou\'s MultiEpoch](https://zhuanlan.zhihu.com/p/669063912)"},{"id":"/AppS/intro","metadata":{"permalink":"/blog/AppS/intro","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/AppS/intro.md","source":"@site/blog/AppS/intro.md","title":"Applicaton Store(AppS) Introduction","description":"Apps functions as a digital distribution platform similar to Apple\'s AppStore or Google Play, focusing on providing users with a variety of applications and games. Our primary key performance indicator is the number of user downloads and installations, which directly reflects the platform\'s usage and user satisfaction.","date":"2025-03-22T08:54:54.000Z","tags":[],"readingTime":1.415,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"The One Epoch Phenomenon in Recommendation Algorithms","permalink":"/blog/AppS/OneEpochInRecAlgo"}},"content":"Apps functions as a digital distribution platform similar to Apple\'s AppStore or Google Play, focusing on providing users with a variety of applications and games. Our primary key performance indicator is the number of user downloads and installations, which directly reflects the platform\'s usage and user satisfaction.\\n\\n<img title=\\"\\" src=\\"../../static/images/appstore.jpg\\" alt=\\"\\" width=\\"476\\" data-align=\\"center\\">\\n\\n<img title=\\"\\" src=\\"../../static/images/googleplay.png\\" alt=\\"\\" width=\\"476\\" data-align=\\"center\\">\\n\\n**Current Status**\\n\\n1. **Network Adaptability**  \\n   In situations of no network or weak network, this platform utilizes cached information to ensure that the user interface remains fully populated, avoiding blank screens. This feature enhances user experience under various network conditions.\\n\\n2. **App and Game Distribution**  \\n   The platform primarily distributes apps and games. With the expertise of our operational staff, these are configured for placement on different leaderboards. However, as these leaderboards are manually operated, the configuration tasks are heavy, with the homepage alone featuring more than 20 leaderboards. This results in longer onboarding cycles, typically requiring two weeks for review. This manual configuration approach leads to slow leaderboard updates and poor performance of new apps during the cold start phase, reducing overall distribution efficiency.\\n\\n3. **Performance of \\"Guess You Like\\" Section**  \\n   The \\"You May Like\\" section of the platform is a relatively high-traffic area, accounting for **45%** of the platform\'s daily active users, yet it only contributes **5%** of the total downloads. Despite the high traffic, the limited exposure slots result in download volumes not matching the expected contribution levels.\\n\\n**Future Goals**\\n\\nTo enhance the number of downloads and installations, the platform plans to integrate an advanced recommendation system. Through intelligent recommendations, we aim to effectively expedite leaderboard updates and improve the exposure of new applications, thereby enhancing overall distribution efficiency and optimizing user experience."}]}}')}}]);